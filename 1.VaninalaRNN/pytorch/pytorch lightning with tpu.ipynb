{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNOdUQRWl5MbvOpl5Yut4Pz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PapuZE0RS8X6"},"outputs":[],"source":["#! pip install --quiet \"pytorch-lightning>=2.0, <2.1.0\" \"matplotlib\" \"numpy <2.0\" \"torchvision\" \"torchmetrics>=1.0, <1.3\" \"torch>=1.8.1, <2.1.0\""]},{"cell_type":"code","source":["from torchmetrics.functional import accuracy\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TrBe4K8YUgZF","executionInfo":{"status":"ok","timestamp":1736934200429,"user_tz":-360,"elapsed":5711,"user":{"displayName":"jabed omor","userId":"16128518018356494995"}},"outputId":"196d5b62-dda6-4c62-cdda-ec80dc8002ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["#! pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl\n","# !pip install pytorch-lightning\n","import pytorch_lightning as pl"],"metadata":{"id":"QZEffjgyTEsX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from torch.utils.data import DataLoader, random_split\n","\n","from torchvision import transforms\n","\n","# Note - you must have torchvision installed for this example\n","from torchvision.datasets import MNIST\n","\n","BATCH_SIZE = 128"],"metadata":{"id":"EOXSQy6eTQM9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MNISTDataModule(pl.LightningDataModule):\n","    def __init__(self, data_dir: str = \"./\"):\n","        super().__init__()\n","        self.data_dir = data_dir\n","        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n","\n","        self.dims = (1, 28, 28)\n","        self.num_classes = 10\n","\n","    def prepare_data(self):\n","        # download\n","        MNIST(self.data_dir, train=True, download=True)\n","        MNIST(self.data_dir, train=False, download=True)\n","\n","    def setup(self, stage=None):\n","        # Assign train/val datasets for use in dataloaders\n","        if stage == \"fit\" or stage is None:\n","            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n","            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n","\n","        # Assign test dataset for use in dataloader(s)\n","        if stage == \"test\" or stage is None:\n","            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.mnist_train, batch_size=BATCH_SIZE)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.mnist_val, batch_size=BATCH_SIZE)\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.mnist_test, batch_size=BATCH_SIZE)"],"metadata":{"id":"kT2m2HgeTSjU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LitModel(pl.LightningModule):\n","    def __init__(self, channels, width, height, num_classes, hidden_size=64, learning_rate=2e-4):\n","        super().__init__()\n","\n","        self.save_hyperparameters()\n","\n","        self.model = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(channels * width * height, hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(hidden_size, num_classes),\n","        )\n","\n","    def forward(self, x):\n","        x = self.model(x)\n","        return F.log_softmax(x, dim=1)\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.nll_loss(logits, y)\n","        self.log(\"train_loss\", loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = F.nll_loss(logits, y)\n","        preds = torch.argmax(logits, dim=1)\n","        acc = accuracy(preds, y)\n","        self.log(\"val_loss\", loss, prog_bar=True)\n","        self.log(\"val_acc\", acc, prog_bar=True)\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n","        return optimizer"],"metadata":{"id":"KujgbhNlTdOy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Init DataModule\n","# dm = MNISTDataModule()\n","# # Init model from datamodule's attributes\n","# model = LitModel(*dm.size(), dm.num_classes)\n","# # Init trainer\n","# trainer = pl.Trainer(\n","#     max_epochs=3,\n","#     accelerator=\"tpu\",\n","#     devices=[5],\n","# )\n","# # Train\n","# trainer.fit(model, dm)"],"metadata":{"id":"8DFKbCDtTgA8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Init DataModule\n","dm = MNISTDataModule()\n","# Init model from datamodule's attributes\n","model = LitModel(*dm.dims, dm.num_classes)\n","# Init trainer\n","trainer = pl.Trainer(\n","    max_epochs=3,\n","    accelerator=\"tpu\",\n","    devices=1,\n",")\n","# Train\n","trainer.fit(model, dm)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZvWT7HNxTlp1","outputId":"e9032adf-4bfe-4ce1-fdcb-c6fe960101e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: True, using: 1 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]}]},{"cell_type":"code","source":["import torch\n","import torch_xla\n","import torch_xla.core.xla_model as xm\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sih6nzk2VJg-","executionInfo":{"status":"ok","timestamp":1736934243491,"user_tz":-360,"elapsed":4116,"user":{"displayName":"jabed omor","userId":"16128518018356494995"}},"outputId":"2e971eb5-f9ae-4157-a9f5-78dbe61e3327"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["t = torch.randn(2, 2, device=xm.xla_device())\n","print(t.device)\n","print(t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9BZzxjKKWSn8","executionInfo":{"status":"ok","timestamp":1736934267710,"user_tz":-360,"elapsed":15270,"user":{"displayName":"jabed omor","userId":"16128518018356494995"}},"outputId":"728b15ee-7aaa-4ef8-a465-b1a2d2933bab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["xla:0\n","tensor([[-1.2189,  0.1811],\n","        [-0.4774, -1.9252]], device='xla:0')\n"]}]},{"cell_type":"code","source":["t0 = torch.randn(2, 2, device=xm.xla_device())\n","t1 = torch.randn(2, 2, device=xm.xla_device())\n","print(t0 + t1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZTPn4p1NWVzY","executionInfo":{"status":"ok","timestamp":1736934294372,"user_tz":-360,"elapsed":436,"user":{"displayName":"jabed omor","userId":"16128518018356494995"}},"outputId":"d41c630b-b64f-46bf-f237-e047497fc3d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.3573, -0.1785],\n","        [-1.1979, -1.2159]], device='xla:0')\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from torch.utils.data import DataLoader, random_split\n","\n","from torchvision import transforms\n","\n","# Note - you must have torchvision installed for this example\n","from torchvision.datasets import MNIST\n","\n","BATCH_SIZE = 128"],"metadata":{"id":"0lbOuWV3XSDN","executionInfo":{"status":"ok","timestamp":1737008633962,"user_tz":-360,"elapsed":1650,"user":{"displayName":"jabed omor","userId":"16128518018356494995"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","# Define transformations for MNIST (normalizing pixel values)\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))  # Mean and standard deviation for normalization\n","])\n","\n","# Load MNIST dataset\n","data_ = \"./data\"  # Directory to save MNIST data\n","mnist_train = datasets.MNIST(data_, train=True, download=True, transform=transform)\n","\n","# Create DataLoader\n","\n","\n","# Check the DataLoader\n","data_iter = iter(train_loader)\n","images, labels = next(data_iter)\n","print(f\"Batch size: {images.size()} Labels: {labels.size()}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3WXCMq5VXhSw","executionInfo":{"status":"ok","timestamp":1737009173756,"user_tz":-360,"elapsed":523366,"user":{"displayName":"jabed omor","userId":"16128518018356494995"}},"outputId":"48818069-2118-4f2f-c298-5ff7d7fc6fe8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","<urlopen error [Errno 110] Connection timed out>\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 41.9MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","<urlopen error [Errno 110] Connection timed out>\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28.9k/28.9k [00:00<00:00, 1.08MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","<urlopen error [Errno 110] Connection timed out>\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1.65M/1.65M [00:00<00:00, 10.1MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","<urlopen error [Errno 110] Connection timed out>\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4.54k/4.54k [00:00<00:00, 5.16MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Batch size: torch.Size([128, 1, 28, 28]) Labels: torch.Size([128])\n"]}]},{"cell_type":"code","source":["train_loader"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mGpulzQNyKAW","executionInfo":{"status":"ok","timestamp":1737009175139,"user_tz":-360,"elapsed":17,"user":{"displayName":"jabed omor","userId":"16128518018356494995"}},"outputId":"ae9f08fc-6e4b-4991-e2e0-acc0134e810c"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.utils.data.dataloader.DataLoader at 0x7aba694bc190>"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Define the MNIST model\n","class MNISTModel(nn.Module):\n","    def __init__(self):\n","        super(MNISTModel, self).__init__()\n","        # Define layers\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)  # First convolution\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) # Second convolution\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Max-pooling\n","        self.fc1 = nn.Linear(64 * 14 * 14, 128)  # Fully connected layer\n","        self.fc2 = nn.Linear(128, 10)  # Output layer for 10 classes (digits 0-9)\n","\n","    def forward(self, x):\n","        # Define forward pass\n","        x = F.relu(self.conv1(x))  # Apply ReLU activation to first conv layer\n","        x = self.pool(F.relu(self.conv2(x)))  # Apply ReLU and pool for the second layer\n","        x = x.view(-1, 64 * 14 * 14)  # Flatten\n","        x = F.relu(self.fc1(x))  # Fully connected with ReLU\n","        x = self.fc2(x)  # Output layer\n","        return x\n","\n","# Instantiate the model\n","model = MNISTModel()\n","print(model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2M-eUaMLYWj0","executionInfo":{"status":"ok","timestamp":1737009561114,"user_tz":-360,"elapsed":430,"user":{"displayName":"jabed omor","userId":"16128518018356494995"}},"outputId":"676a3132-0e14-4d88-dded-3c487d9909dc"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["MNISTModel(\n","  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=12544, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["import torch_xla.core.xla_model as xm\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","\n","train_loader = DataLoader(mnist_train, batch_size=128, shuffle=True)\n","# Create model and move to device\n","\n","start=time.time()\n","device = xm.xla_device()\n","model = MNISTModel().train().to(device)\n","\n","# Use CrossEntropyLoss for multi-class classification\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Using Adam optimizer for better convergence\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","for epoch in range(5):  # Number of epochs\n","    for data, target in train_loader:\n","        optimizer.zero_grad()  # Zero gradients\n","        data = data.to(device)  # Move data to TPU device\n","        target = target.to(device)  # Move target to TPU device\n","\n","        # Forward pass\n","        output = model(data)\n","\n","        # Compute loss\n","        loss = loss_fn(output, target)\n","\n","        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n","\n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Mark step for TPU\n","        xm.mark_step()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"J8lgdoKFWf8y","executionInfo":{"status":"error","timestamp":1737009675963,"user_tz":-360,"elapsed":111171,"user":{"displayName":"jabed omor","userId":"16128518018356494995"}},"outputId":"092761ad-ddb7-47e1-af54-94ad707a5bd1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 2.30381441116333\n","Epoch 1, Loss: 2.315786838531494\n","Epoch 1, Loss: 2.2424440383911133\n","Epoch 1, Loss: 2.0761687755584717\n","Epoch 1, Loss: 2.012056827545166\n","Epoch 1, Loss: 1.8905925750732422\n","Epoch 1, Loss: 1.6582918167114258\n","Epoch 1, Loss: 1.5120315551757812\n","Epoch 1, Loss: 1.292862892150879\n","Epoch 1, Loss: 1.1775978803634644\n","Epoch 1, Loss: 0.9213013052940369\n","Epoch 1, Loss: 0.8326495885848999\n","Epoch 1, Loss: 0.5931951403617859\n","Epoch 1, Loss: 0.7363439202308655\n","Epoch 1, Loss: 0.557392418384552\n","Epoch 1, Loss: 0.46389928460121155\n","Epoch 1, Loss: 0.6975029706954956\n","Epoch 1, Loss: 0.707134485244751\n","Epoch 1, Loss: 0.4261834919452667\n","Epoch 1, Loss: 0.3734343647956848\n","Epoch 1, Loss: 0.41023528575897217\n","Epoch 1, Loss: 0.512501060962677\n","Epoch 1, Loss: 0.7084401249885559\n","Epoch 1, Loss: 0.4204099774360657\n","Epoch 1, Loss: 0.4960389733314514\n","Epoch 1, Loss: 0.5962305068969727\n","Epoch 1, Loss: 0.4955253303050995\n","Epoch 1, Loss: 0.5850709080696106\n","Epoch 1, Loss: 0.3959256708621979\n","Epoch 1, Loss: 0.3746831715106964\n","Epoch 1, Loss: 0.3618004620075226\n","Epoch 1, Loss: 0.36168864369392395\n","Epoch 1, Loss: 0.23964393138885498\n","Epoch 1, Loss: 0.46442151069641113\n","Epoch 1, Loss: 0.3953062891960144\n","Epoch 1, Loss: 0.4180664122104645\n","Epoch 1, Loss: 0.19887591898441315\n","Epoch 1, Loss: 0.2654970586299896\n","Epoch 1, Loss: 0.2224382758140564\n","Epoch 1, Loss: 0.23844905197620392\n","Epoch 1, Loss: 0.2308243364095688\n","Epoch 1, Loss: 0.37185484170913696\n","Epoch 1, Loss: 0.430193692445755\n","Epoch 1, Loss: 0.23339027166366577\n","Epoch 1, Loss: 0.4198823571205139\n","Epoch 1, Loss: 0.36096957325935364\n","Epoch 1, Loss: 0.19924236834049225\n","Epoch 1, Loss: 0.3690468966960907\n","Epoch 1, Loss: 0.19468387961387634\n","Epoch 1, Loss: 0.2708964943885803\n","Epoch 1, Loss: 0.25550755858421326\n","Epoch 1, Loss: 0.278305321931839\n","Epoch 1, Loss: 0.4259811341762543\n","Epoch 1, Loss: 0.34771379828453064\n","Epoch 1, Loss: 0.26142555475234985\n","Epoch 1, Loss: 0.3349394202232361\n","Epoch 1, Loss: 0.25165843963623047\n","Epoch 1, Loss: 0.2423906922340393\n","Epoch 1, Loss: 0.19938547909259796\n","Epoch 1, Loss: 0.19605661928653717\n","Epoch 1, Loss: 0.23388910293579102\n","Epoch 1, Loss: 0.28032350540161133\n","Epoch 1, Loss: 0.23210643231868744\n","Epoch 1, Loss: 0.12698042392730713\n","Epoch 1, Loss: 0.28950056433677673\n","Epoch 1, Loss: 0.22049033641815186\n","Epoch 1, Loss: 0.2213248759508133\n","Epoch 1, Loss: 0.1787288635969162\n","Epoch 1, Loss: 0.2639257609844208\n","Epoch 1, Loss: 0.1498589962720871\n","Epoch 1, Loss: 0.13399916887283325\n","Epoch 1, Loss: 0.251089870929718\n","Epoch 1, Loss: 0.19896015524864197\n","Epoch 1, Loss: 0.26019424200057983\n","Epoch 1, Loss: 0.10325636714696884\n","Epoch 1, Loss: 0.2679978311061859\n","Epoch 1, Loss: 0.18860745429992676\n","Epoch 1, Loss: 0.1643819659948349\n","Epoch 1, Loss: 0.19959339499473572\n","Epoch 1, Loss: 0.1152118518948555\n","Epoch 1, Loss: 0.17096230387687683\n","Epoch 1, Loss: 0.1123628318309784\n","Epoch 1, Loss: 0.11452632397413254\n","Epoch 1, Loss: 0.17653192579746246\n","Epoch 1, Loss: 0.1594988852739334\n","Epoch 1, Loss: 0.24744176864624023\n","Epoch 1, Loss: 0.1322673112154007\n","Epoch 1, Loss: 0.1536773443222046\n","Epoch 1, Loss: 0.18177518248558044\n","Epoch 1, Loss: 0.11865641921758652\n","Epoch 1, Loss: 0.21197852492332458\n","Epoch 1, Loss: 0.17448286712169647\n","Epoch 1, Loss: 0.22197522222995758\n","Epoch 1, Loss: 0.18892541527748108\n","Epoch 1, Loss: 0.15329307317733765\n","Epoch 1, Loss: 0.20250001549720764\n","Epoch 1, Loss: 0.15372870862483978\n","Epoch 1, Loss: 0.1519349068403244\n","Epoch 1, Loss: 0.13277028501033783\n","Epoch 1, Loss: 0.16802175343036652\n","Epoch 1, Loss: 0.16540713608264923\n","Epoch 1, Loss: 0.21064847707748413\n","Epoch 1, Loss: 0.1349579244852066\n","Epoch 1, Loss: 0.14020481705665588\n","Epoch 1, Loss: 0.1705252230167389\n","Epoch 1, Loss: 0.10821697860956192\n","Epoch 1, Loss: 0.17572550475597382\n","Epoch 1, Loss: 0.1275801658630371\n","Epoch 1, Loss: 0.11797211319208145\n","Epoch 1, Loss: 0.11610766500234604\n","Epoch 1, Loss: 0.09146908670663834\n","Epoch 1, Loss: 0.111970454454422\n","Epoch 1, Loss: 0.14013533294200897\n","Epoch 1, Loss: 0.08594779670238495\n","Epoch 1, Loss: 0.10956418514251709\n","Epoch 1, Loss: 0.08165118098258972\n","Epoch 1, Loss: 0.12146886438131332\n","Epoch 1, Loss: 0.2299734205007553\n","Epoch 1, Loss: 0.0876624807715416\n","Epoch 1, Loss: 0.08904417604207993\n","Epoch 1, Loss: 0.08401355147361755\n","Epoch 1, Loss: 0.09460297226905823\n","Epoch 1, Loss: 0.1488385647535324\n","Epoch 1, Loss: 0.11348206549882889\n","Epoch 1, Loss: 0.06041846424341202\n","Epoch 1, Loss: 0.23510825634002686\n","Epoch 1, Loss: 0.055439162999391556\n","Epoch 1, Loss: 0.1340874284505844\n","Epoch 1, Loss: 0.07713037729263306\n","Epoch 1, Loss: 0.1465967893600464\n","Epoch 1, Loss: 0.05644877254962921\n","Epoch 1, Loss: 0.08300920575857162\n","Epoch 1, Loss: 0.048811111599206924\n","Epoch 1, Loss: 0.15867389738559723\n","Epoch 1, Loss: 0.18936210870742798\n","Epoch 1, Loss: 0.11569975316524506\n","Epoch 1, Loss: 0.10093264281749725\n","Epoch 1, Loss: 0.07328087836503983\n","Epoch 1, Loss: 0.1063285544514656\n","Epoch 1, Loss: 0.12747211754322052\n","Epoch 1, Loss: 0.1731424331665039\n","Epoch 1, Loss: 0.15237659215927124\n","Epoch 1, Loss: 0.1902802735567093\n","Epoch 1, Loss: 0.2601023018360138\n","Epoch 1, Loss: 0.17646144330501556\n","Epoch 1, Loss: 0.07979423552751541\n","Epoch 1, Loss: 0.1349848061800003\n","Epoch 1, Loss: 0.1183868870139122\n","Epoch 1, Loss: 0.08288054913282394\n","Epoch 1, Loss: 0.13694576919078827\n","Epoch 1, Loss: 0.1151604875922203\n","Epoch 1, Loss: 0.16360563039779663\n","Epoch 1, Loss: 0.11098165065050125\n","Epoch 1, Loss: 0.11919217556715012\n","Epoch 1, Loss: 0.1223541796207428\n","Epoch 1, Loss: 0.13924536108970642\n","Epoch 1, Loss: 0.24404361844062805\n","Epoch 1, Loss: 0.14435935020446777\n","Epoch 1, Loss: 0.07607044279575348\n","Epoch 1, Loss: 0.13247549533843994\n","Epoch 1, Loss: 0.0751168355345726\n","Epoch 1, Loss: 0.17591144144535065\n","Epoch 1, Loss: 0.14460815489292145\n","Epoch 1, Loss: 0.10296596586704254\n","Epoch 1, Loss: 0.11929289251565933\n","Epoch 1, Loss: 0.11034208536148071\n","Epoch 1, Loss: 0.11275173723697662\n","Epoch 1, Loss: 0.08547157794237137\n","Epoch 1, Loss: 0.12392402440309525\n","Epoch 1, Loss: 0.06706184893846512\n","Epoch 1, Loss: 0.20226038992404938\n","Epoch 1, Loss: 0.07752137631177902\n","Epoch 1, Loss: 0.059216033667325974\n","Epoch 1, Loss: 0.14419816434383392\n","Epoch 1, Loss: 0.06427863240242004\n","Epoch 1, Loss: 0.08624927699565887\n","Epoch 1, Loss: 0.10671263188123703\n","Epoch 1, Loss: 0.024612590670585632\n","Epoch 1, Loss: 0.09298201650381088\n","Epoch 1, Loss: 0.1383393257856369\n","Epoch 1, Loss: 0.05700422078371048\n","Epoch 1, Loss: 0.1596309393644333\n","Epoch 1, Loss: 0.04233910143375397\n","Epoch 1, Loss: 0.0811474621295929\n","Epoch 1, Loss: 0.09658417105674744\n","Epoch 1, Loss: 0.11004272848367691\n","Epoch 1, Loss: 0.09668753296136856\n","Epoch 1, Loss: 0.11105838418006897\n","Epoch 1, Loss: 0.017546476796269417\n","Epoch 1, Loss: 0.037714406847953796\n","Epoch 1, Loss: 0.032319094985723495\n","Epoch 1, Loss: 0.18457037210464478\n","Epoch 1, Loss: 0.13884276151657104\n","Epoch 1, Loss: 0.08301351219415665\n","Epoch 1, Loss: 0.07020622491836548\n","Epoch 1, Loss: 0.12271563708782196\n","Epoch 1, Loss: 0.09369299560785294\n","Epoch 1, Loss: 0.11540353298187256\n","Epoch 1, Loss: 0.1086052805185318\n","Epoch 1, Loss: 0.07562343031167984\n","Epoch 1, Loss: 0.10278258472681046\n","Epoch 1, Loss: 0.07695107161998749\n","Epoch 1, Loss: 0.05245798081159592\n","Epoch 1, Loss: 0.14213143289089203\n","Epoch 1, Loss: 0.07872729003429413\n","Epoch 1, Loss: 0.12166482955217361\n","Epoch 1, Loss: 0.17113745212554932\n","Epoch 1, Loss: 0.18252870440483093\n","Epoch 1, Loss: 0.1092868521809578\n","Epoch 1, Loss: 0.07272780686616898\n","Epoch 1, Loss: 0.09212184697389603\n","Epoch 1, Loss: 0.0727740079164505\n","Epoch 1, Loss: 0.0913737490773201\n","Epoch 1, Loss: 0.08766650408506393\n","Epoch 1, Loss: 0.1170467883348465\n","Epoch 1, Loss: 0.10937711596488953\n","Epoch 1, Loss: 0.13023792207241058\n","Epoch 1, Loss: 0.14465059340000153\n","Epoch 1, Loss: 0.03660063445568085\n","Epoch 1, Loss: 0.08259989321231842\n","Epoch 1, Loss: 0.13833655416965485\n","Epoch 1, Loss: 0.09187991917133331\n","Epoch 1, Loss: 0.12377406656742096\n","Epoch 1, Loss: 0.10243454575538635\n","Epoch 1, Loss: 0.08985372632741928\n","Epoch 1, Loss: 0.10197699069976807\n","Epoch 1, Loss: 0.1551651805639267\n","Epoch 1, Loss: 0.035287823528051376\n","Epoch 1, Loss: 0.047182533890008926\n","Epoch 1, Loss: 0.10910011827945709\n","Epoch 1, Loss: 0.2806210517883301\n","Epoch 1, Loss: 0.10297956317663193\n","Epoch 1, Loss: 0.06012836471199989\n","Epoch 1, Loss: 0.1664293259382248\n","Epoch 1, Loss: 0.13969136774539948\n","Epoch 1, Loss: 0.101518414914608\n","Epoch 1, Loss: 0.07526059448719025\n","Epoch 1, Loss: 0.1182725578546524\n","Epoch 1, Loss: 0.1303122639656067\n","Epoch 1, Loss: 0.1447804868221283\n","Epoch 1, Loss: 0.06598664075136185\n","Epoch 1, Loss: 0.12233111262321472\n","Epoch 1, Loss: 0.09148017317056656\n","Epoch 1, Loss: 0.08132606744766235\n","Epoch 1, Loss: 0.05404071882367134\n","Epoch 1, Loss: 0.12388724088668823\n","Epoch 1, Loss: 0.10033855587244034\n","Epoch 1, Loss: 0.07177653908729553\n","Epoch 1, Loss: 0.07712481915950775\n","Epoch 1, Loss: 0.0352245457470417\n","Epoch 1, Loss: 0.05160604044795036\n","Epoch 1, Loss: 0.08393710851669312\n","Epoch 1, Loss: 0.09713149070739746\n","Epoch 1, Loss: 0.13428014516830444\n","Epoch 1, Loss: 0.11973695456981659\n","Epoch 1, Loss: 0.09214542806148529\n","Epoch 1, Loss: 0.11053458601236343\n","Epoch 1, Loss: 0.16074541211128235\n","Epoch 1, Loss: 0.1016671434044838\n","Epoch 1, Loss: 0.07875993102788925\n","Epoch 1, Loss: 0.05407300963997841\n","Epoch 1, Loss: 0.04679524153470993\n","Epoch 1, Loss: 0.06795808672904968\n","Epoch 1, Loss: 0.09223003685474396\n","Epoch 1, Loss: 0.10675248503684998\n","Epoch 1, Loss: 0.07524598389863968\n","Epoch 1, Loss: 0.06629260629415512\n","Epoch 1, Loss: 0.06413337588310242\n","Epoch 1, Loss: 0.07555146515369415\n","Epoch 1, Loss: 0.10429943352937698\n","Epoch 1, Loss: 0.13570278882980347\n","Epoch 1, Loss: 0.12148943543434143\n","Epoch 1, Loss: 0.12724189460277557\n","Epoch 1, Loss: 0.10302115231752396\n","Epoch 1, Loss: 0.10195896029472351\n","Epoch 1, Loss: 0.06758695095777512\n","Epoch 1, Loss: 0.12557469308376312\n","Epoch 1, Loss: 0.09488114714622498\n","Epoch 1, Loss: 0.11450265347957611\n","Epoch 1, Loss: 0.046935517340898514\n","Epoch 1, Loss: 0.08509741723537445\n","Epoch 1, Loss: 0.1544482558965683\n","Epoch 1, Loss: 0.055336467921733856\n","Epoch 1, Loss: 0.14846792817115784\n","Epoch 1, Loss: 0.052167654037475586\n","Epoch 1, Loss: 0.10129860788583755\n","Epoch 1, Loss: 0.04742215946316719\n","Epoch 1, Loss: 0.11620987951755524\n","Epoch 1, Loss: 0.10798709094524384\n","Epoch 1, Loss: 0.10046679526567459\n","Epoch 1, Loss: 0.060579851269721985\n","Epoch 1, Loss: 0.06825976073741913\n","Epoch 1, Loss: 0.057111866772174835\n","Epoch 1, Loss: 0.05692775547504425\n","Epoch 1, Loss: 0.030087163671851158\n","Epoch 1, Loss: 0.029846003279089928\n","Epoch 1, Loss: 0.05323188379406929\n","Epoch 1, Loss: 0.06915958225727081\n","Epoch 1, Loss: 0.032152462750673294\n","Epoch 1, Loss: 0.0898299589753151\n","Epoch 1, Loss: 0.0941711813211441\n","Epoch 1, Loss: 0.03114756941795349\n","Epoch 1, Loss: 0.04192249849438667\n","Epoch 1, Loss: 0.05974409356713295\n","Epoch 1, Loss: 0.0881497859954834\n","Epoch 1, Loss: 0.03451775386929512\n","Epoch 1, Loss: 0.09266374260187149\n","Epoch 1, Loss: 0.09187678247690201\n","Epoch 1, Loss: 0.02813190408051014\n","Epoch 1, Loss: 0.0709604024887085\n","Epoch 1, Loss: 0.05809452757239342\n","Epoch 1, Loss: 0.1013331338763237\n","Epoch 1, Loss: 0.01432492770254612\n","Epoch 1, Loss: 0.11161934584379196\n","Epoch 1, Loss: 0.04957732558250427\n","Epoch 1, Loss: 0.061534710228443146\n","Epoch 1, Loss: 0.13119810819625854\n","Epoch 1, Loss: 0.058392804116010666\n","Epoch 1, Loss: 0.023969238623976707\n","Epoch 1, Loss: 0.07102818787097931\n","Epoch 1, Loss: 0.11854474246501923\n","Epoch 1, Loss: 0.1044631227850914\n","Epoch 1, Loss: 0.09052900224924088\n","Epoch 1, Loss: 0.10596796870231628\n","Epoch 1, Loss: 0.08940617740154266\n","Epoch 1, Loss: 0.1085323765873909\n","Epoch 1, Loss: 0.09300632774829865\n","Epoch 1, Loss: 0.05300390347838402\n","Epoch 1, Loss: 0.04310724139213562\n","Epoch 1, Loss: 0.07201122492551804\n","Epoch 1, Loss: 0.0675085037946701\n","Epoch 1, Loss: 0.07352244853973389\n","Epoch 1, Loss: 0.07460376620292664\n","Epoch 1, Loss: 0.16018953919410706\n","Epoch 1, Loss: 0.1123037114739418\n","Epoch 1, Loss: 0.06980744749307632\n","Epoch 1, Loss: 0.021352339535951614\n","Epoch 1, Loss: 0.10074640810489655\n","Epoch 1, Loss: 0.09373947978019714\n","Epoch 1, Loss: 0.09937771409749985\n","Epoch 1, Loss: 0.07467132806777954\n","Epoch 1, Loss: 0.0403551310300827\n","Epoch 1, Loss: 0.07696684449911118\n","Epoch 1, Loss: 0.0489443764090538\n","Epoch 1, Loss: 0.06871819496154785\n","Epoch 1, Loss: 0.1140844002366066\n","Epoch 1, Loss: 0.0886361375451088\n","Epoch 1, Loss: 0.05782698467373848\n","Epoch 1, Loss: 0.09203390032052994\n","Epoch 1, Loss: 0.10175380855798721\n","Epoch 1, Loss: 0.0705718994140625\n","Epoch 1, Loss: 0.03392042964696884\n","Epoch 1, Loss: 0.037683434784412384\n","Epoch 1, Loss: 0.07793570309877396\n","Epoch 1, Loss: 0.019374145194888115\n","Epoch 1, Loss: 0.050532933324575424\n","Epoch 1, Loss: 0.09893855452537537\n","Epoch 1, Loss: 0.01649099960923195\n","Epoch 1, Loss: 0.03076338954269886\n","Epoch 1, Loss: 0.11009927839040756\n","Epoch 1, Loss: 0.0760793387889862\n","Epoch 1, Loss: 0.03369112312793732\n","Epoch 1, Loss: 0.16566385328769684\n","Epoch 1, Loss: 0.10628356039524078\n","Epoch 1, Loss: 0.09473143517971039\n","Epoch 1, Loss: 0.09166739881038666\n","Epoch 1, Loss: 0.019086681306362152\n","Epoch 1, Loss: 0.03873927518725395\n","Epoch 1, Loss: 0.07020014524459839\n","Epoch 1, Loss: 0.05111255496740341\n","Epoch 1, Loss: 0.059203457087278366\n","Epoch 1, Loss: 0.017752228304743767\n","Epoch 1, Loss: 0.09137877821922302\n","Epoch 1, Loss: 0.05162443220615387\n","Epoch 1, Loss: 0.15479908883571625\n","Epoch 1, Loss: 0.06046297773718834\n","Epoch 1, Loss: 0.16387757658958435\n","Epoch 1, Loss: 0.1149098128080368\n","Epoch 1, Loss: 0.029328735545277596\n","Epoch 1, Loss: 0.05772172287106514\n","Epoch 1, Loss: 0.07780954241752625\n","Epoch 1, Loss: 0.0583895780146122\n","Epoch 1, Loss: 0.07488277554512024\n","Epoch 1, Loss: 0.1061042919754982\n","Epoch 1, Loss: 0.14710702002048492\n","Epoch 1, Loss: 0.09458010643720627\n","Epoch 1, Loss: 0.05389256775379181\n","Epoch 1, Loss: 0.08306097984313965\n","Epoch 1, Loss: 0.08672462403774261\n","Epoch 1, Loss: 0.04163770750164986\n","Epoch 1, Loss: 0.03321664780378342\n","Epoch 1, Loss: 0.01888282783329487\n","Epoch 1, Loss: 0.09137114137411118\n","Epoch 1, Loss: 0.08323543518781662\n","Epoch 1, Loss: 0.02594243921339512\n","Epoch 1, Loss: 0.0575498566031456\n","Epoch 1, Loss: 0.06246776878833771\n","Epoch 1, Loss: 0.035429853945970535\n","Epoch 1, Loss: 0.04681609570980072\n","Epoch 1, Loss: 0.07667531073093414\n","Epoch 1, Loss: 0.11335189640522003\n","Epoch 1, Loss: 0.07433993369340897\n","Epoch 1, Loss: 0.039862025529146194\n","Epoch 1, Loss: 0.05292436480522156\n","Epoch 1, Loss: 0.06502323597669601\n","Epoch 1, Loss: 0.05528043955564499\n","Epoch 1, Loss: 0.036028310656547546\n","Epoch 1, Loss: 0.029510218650102615\n","Epoch 1, Loss: 0.03841383382678032\n","Epoch 1, Loss: 0.12457223236560822\n","Epoch 1, Loss: 0.03970547765493393\n","Epoch 1, Loss: 0.0269001517444849\n","Epoch 1, Loss: 0.08118382841348648\n","Epoch 1, Loss: 0.0601324699819088\n","Epoch 1, Loss: 0.04871561750769615\n","Epoch 1, Loss: 0.04247692972421646\n","Epoch 1, Loss: 0.06617773324251175\n","Epoch 1, Loss: 0.0990019291639328\n","Epoch 1, Loss: 0.028218509629368782\n","Epoch 1, Loss: 0.018624966964125633\n","Epoch 1, Loss: 0.03139646723866463\n","Epoch 1, Loss: 0.10405193269252777\n","Epoch 1, Loss: 0.03365226089954376\n","Epoch 1, Loss: 0.06663108617067337\n","Epoch 1, Loss: 0.04570208489894867\n","Epoch 1, Loss: 0.09836114197969437\n","Epoch 1, Loss: 0.2421298772096634\n","Epoch 1, Loss: 0.09794992953538895\n","Epoch 1, Loss: 0.11945485323667526\n","Epoch 1, Loss: 0.04102309048175812\n","Epoch 1, Loss: 0.033275481313467026\n","Epoch 1, Loss: 0.09415959566831589\n","Epoch 1, Loss: 0.022770335897803307\n","Epoch 1, Loss: 0.10631615668535233\n","Epoch 1, Loss: 0.049958109855651855\n","Epoch 1, Loss: 0.14240863919258118\n","Epoch 1, Loss: 0.0987347662448883\n","Epoch 1, Loss: 0.07419396191835403\n","Epoch 1, Loss: 0.019949911162257195\n","Epoch 1, Loss: 0.018858570605516434\n","Epoch 1, Loss: 0.054196327924728394\n","Epoch 1, Loss: 0.021247070282697678\n","Epoch 1, Loss: 0.10316842049360275\n","Epoch 1, Loss: 0.02612461894750595\n","Epoch 1, Loss: 0.060240887105464935\n","Epoch 1, Loss: 0.055066682398319244\n","Epoch 1, Loss: 0.030203578993678093\n","Epoch 1, Loss: 0.04135528951883316\n","Epoch 1, Loss: 0.04491018131375313\n","Epoch 1, Loss: 0.13041599094867706\n","Epoch 1, Loss: 0.02414264902472496\n","Epoch 1, Loss: 0.057895727455616\n","Epoch 1, Loss: 0.021031567826867104\n","Epoch 1, Loss: 0.08790434151887894\n","Epoch 1, Loss: 0.05608201399445534\n","Epoch 1, Loss: 0.09794919937849045\n","Epoch 1, Loss: 0.07438727468252182\n","Epoch 1, Loss: 0.03789075091481209\n","Epoch 1, Loss: 0.038133084774017334\n","Epoch 1, Loss: 0.0873919203877449\n","Epoch 1, Loss: 0.02500864490866661\n","Epoch 1, Loss: 0.08213484287261963\n","Epoch 1, Loss: 0.10041342675685883\n","Epoch 1, Loss: 0.06784039735794067\n","Epoch 1, Loss: 0.15562796592712402\n","Epoch 1, Loss: 0.023694124072790146\n","Epoch 1, Loss: 0.013104107230901718\n","Epoch 1, Loss: 0.033815380185842514\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 16.84M of 16.00M vmem. Exceeded vmem capacity by 856.0K.\n\nProgram vmem requirement 16.84M:\n    scoped           16.84M\n\n  Largest program allocations in vmem:\n\n  1. Size: 6.12M\n     Shape: f32[1605632]{0}\n     Unpadded size: 6.12M\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 6.12M\n     Shape: u8[6422528]{0}\n     Unpadded size: 6.12M\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 3.06M\n     Shape: u8[3211264]{0}\n     Unpadded size: 3.06M\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.45M\n     XLA label: register allocator spill slots call depth 2\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 48.0K\n     Shape: f32[12288]{0}\n     Unpadded size: 48.0K\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 8.0K\n     Shape: u8[8192]{0}\n     Unpadded size: 8.0K\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 8.0K\n     Shape: u8[8192]{0}\n     Unpadded size: 8.0K\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n  8. Size: 4.0K\n     Shape: f32[96]{0:T(1024)}\n     Unpadded size: 384B\n     Extra memory due to padding: 3.6K (10.7x expansion)\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n  9. Size: 2.0K\n     Shape: u8[2048]{0}\n     Unpadded size: 2.0K\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n  10. Size: 1.0K\n     Shape: u8[1024]{0}\n     Unpadded size: 1.0K\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-d4f42d97ff7b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}, Loss: {loss.item()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Backward pass and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 16.84M of 16.00M vmem. Exceeded vmem capacity by 856.0K.\n\nProgram vmem requirement 16.84M:\n    scoped           16.84M\n\n  Largest program allocations in vmem:\n\n  1. Size: 6.12M\n     Shape: f32[1605632]{0}\n     Unpadded size: 6.12M\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 6.12M\n     Shape: u8[6422528]{0}\n     Unpadded size: 6.12M\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 3.06M\n     Shape: u8[3211264]{0}\n     Unpadded size: 3.06M\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.45M\n     XLA label: register allocator spill slots call depth 2\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 48.0K\n     Shape: f32[12288]{0}\n     Unpadded size: 48.0K\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 8.0K\n     Shape: u8[8192]{0}\n     Unpadded size: 8.0K\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 8.0K\n     Shape: u8[8192]{0}\n     Unpadded size: 8.0K\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n  8. Size: 4.0K\n     Shape: f32[96]{0:T(1024)}\n     Unpadded size: 384B\n     Extra memory due to padding: 3.6K (10.7x expansion)\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n  9. Size: 2.0K\n     Shape: u8[2048]{0}\n     Unpadded size: 2.0K\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n  10. Size: 1.0K\n     Shape: u8[1024]{0}\n     Unpadded size: 1.0K\n     XLA label: fusion.35 = fusion(p1.2, p2.3, reshape.59, p4.6, ...(+1)), kind=kOutput, calls=fused_computation.35\n     Allocation type: scoped\n     ==========================\n\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.parallel_loader as pl\n","import torch_xla.distributed.xla_multiprocessing as xmp\n","import time\n","\n","# Define your MNISTModel class\n","# class MNISTModel(nn.Module):\n","#     def __init__(self):\n","#         super(MNISTModel, self).__init__()\n","#         self.flatten = nn.Flatten()\n","#         self.fc = nn.Sequential(\n","#             nn.Linear(28 * 28, 512),\n","#             nn.ReLU(),\n","#             nn.Linear(512, 10)\n","#         )\n","\n","#     def forward(self, x):\n","#         x = self.flatten(x)\n","#         x = self.fc(x)\n","#         return x\n","import torch.nn as nn\n","\n","class MNISTModel(nn.Module):\n","    def __init__(self):\n","        super(MNISTModel, self).__init__()\n","        self.conv_layers = nn.Sequential(\n","            # First convolutional layer\n","            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),  # Output: 32x28x28\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 32x14x14\n","\n","            # Second convolutional layer\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),  # Output: 64x14x14\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 64x7x7\n","        )\n","        self.fc_layers = nn.Sequential(\n","            nn.Flatten(),  # Flatten the output from the convolutional layers\n","            nn.Linear(64 * 7 * 7, 128),  # Fully connected layer\n","            nn.ReLU(),\n","            nn.Linear(128, 10)  # Final output layer\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv_layers(x)  # Apply convolutional layers\n","        x = self.fc_layers(x)    # Apply fully connected layers\n","        return x\n","\n","# Accuracy calculation function\n","def calculate_accuracy(output, target):\n","    _, preds = torch.max(output, 1)  # Get the class with highest score\n","    correct = (preds == target).sum().item()\n","    return correct / len(target)\n","\n","# Training and validation loop\n","def train_and_validate(rank, train_dataset, valid_dataset):\n","    # Initialize data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","    valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)\n","\n","    # Create model and move to device\n","    device = xm.xla_device()\n","    model = MNISTModel().to(device)\n","    loss_fn = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    # Start training\n","    num_epochs = 5\n","    for epoch in range(num_epochs):\n","        # Training phase\n","        model.train()\n","        train_loss = 0\n","        train_correct = 0\n","        total_train_samples = 0\n","\n","        para_loader = pl.ParallelLoader(train_loader, [device])\n","        for data, target in para_loader.per_device_loader(device):\n","            optimizer.zero_grad()\n","\n","            # Move data and target to device\n","            data, target = data.to(device), target.to(device)\n","\n","            # Forward pass\n","            output = model(data)\n","\n","            # Compute loss\n","            loss = loss_fn(output, target)\n","            train_loss += loss.item() * data.size(0)  # Accumulate loss\n","            train_correct += calculate_accuracy(output, target) * data.size(0)\n","            total_train_samples += data.size(0)\n","\n","            # Backward pass and optimizer step\n","            loss.backward()\n","            optimizer.step()\n","            xm.mark_step()  # Mark the optimizer step for TPU\n","\n","        # Calculate average training loss and accuracy\n","        train_loss /= total_train_samples\n","        train_accuracy = train_correct / total_train_samples\n","\n","        # Validation phase\n","        model.eval()\n","        valid_loss = 0\n","        valid_correct = 0\n","        total_valid_samples = 0\n","\n","        para_loader = pl.ParallelLoader(valid_loader, [device])\n","        with torch.no_grad():\n","            for data, target in para_loader.per_device_loader(device):\n","                data, target = data.to(device), target.to(device)\n","                output = model(data)\n","\n","                # Compute loss\n","                loss = loss_fn(output, target)\n","                valid_loss += loss.item() * data.size(0)\n","                valid_correct += calculate_accuracy(output, target) * data.size(0)\n","                total_valid_samples += data.size(0)\n","\n","        # Calculate average validation loss and accuracy\n","        valid_loss /= total_valid_samples\n","        valid_accuracy = valid_correct / total_valid_samples\n","\n","        # Print epoch results\n","        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","        print(f\"  Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n","        print(f\"  Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n","\n","# Main function to handle multiprocessing with TPU\n","def main():\n","    # Load your MNIST datasets\n","    from torchvision import datasets, transforms\n","    transform = transforms.Compose([transforms.ToTensor()])\n","    mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n","    mnist_valid = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n","\n","    # Use XLA multiprocessing for TPU\n","    xmp.spawn(train_and_validate, args=(mnist_train, mnist_valid), nprocs=1, start_method='fork')\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"6QSgDnNOXP25","colab":{"base_uri":"https://localhost:8080/","height":322},"executionInfo":{"status":"error","timestamp":1737010190116,"user_tz":-360,"elapsed":4202,"user":{"displayName":"jabed omor","userId":"16128518018356494995"}},"outputId":"a86dcddc-d860-43aa-be24-2aadb31d8671"},"execution_count":12,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-b8bca9934160>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-b8bca9934160>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# Use XLA multiprocessing for TPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \"\"\"\n\u001b[0;32m---> 37\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mpjrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/_internal/pjrt.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, nprocs, start_method, args)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnprocs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_run_singleprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mnprocs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported nprocs (%d), ignoring...'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/_internal/pjrt.py\u001b[0m in \u001b[0;36m_run_singleprocess\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m   \"\"\"\n\u001b[1;32m     96\u001b[0m   \u001b[0minitialize_singleprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/_internal/pjrt.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_ordinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-b8bca9934160>\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(rank, train_dataset, valid_dataset)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Backward pass and optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mxm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Mark the optimizer step for TPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["!pip install torchmetrics pytorch_lightning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGOyC7px4Do_","executionInfo":{"status":"ok","timestamp":1737010224683,"user_tz":-360,"elapsed":8109,"user":{"displayName":"jabed omor","userId":"16128518018356494995"}},"outputId":"2b23f560-c45b-46d9-b251-16c8b543ce57"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchmetrics\n","  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n","Collecting pytorch_lightning\n","  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cpu)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.67.1)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (6.0.2)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.12.0)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.12.2)\n","Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2022.5.0->pytorch_lightning)\n","  Downloading aiohttp-3.11.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n","Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning)\n","  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n","Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning)\n","  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (24.3.0)\n","Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning)\n","  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n","Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning)\n","  Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n","Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning)\n","  Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n","Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning)\n","  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n","Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n","Downloading aiohttp-3.11.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n","Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n","Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.1/231.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.1/344.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: propcache, multidict, lightning-utilities, frozenlist, aiohappyeyeballs, yarl, aiosignal, torchmetrics, aiohttp, pytorch_lightning\n","Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 frozenlist-1.5.0 lightning-utilities-0.11.9 multidict-6.1.0 propcache-0.2.1 pytorch_lightning-2.5.0.post0 torchmetrics-1.6.1 yarl-1.18.3\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"LA6z047H3tdE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchmetrics import Accuracy\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import pytorch_lightning as pl\n","from pytorch_lightning.strategies import TPUStrategy\n","\n","# Define the LightningModule for MNIST\n","class MNISTModel(pl.LightningModule):\n","    def __init__(self, learning_rate=0.001):\n","        super(MNISTModel, self).__init__()\n","        self.save_hyperparameters()\n","\n","        # Define the model\n","        self.conv_layers = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),  # 32x28x28\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),  # 32x14x14\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),  # 64x14x14\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),  # 64x7x7\n","        )\n","        self.fc_layers = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(64 * 7 * 7, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 10),\n","        )\n","\n","        # Loss function and metric\n","        self.loss_fn = nn.CrossEntropyLoss()\n","        self.accuracy = Accuracy()\n","\n","    def forward(self, x):\n","        x = self.conv_layers(x)\n","        x = self.fc_layers(x)\n","        return x\n","\n","    def training_step(self, batch, batch_idx):\n","        data, target = batch\n","        output = self(data)\n","        loss = self.loss_fn(output, target)\n","        acc = self.accuracy(output.softmax(dim=-1), target)\n","        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n","        self.log(\"train_acc\", acc, prog_bar=True, on_epoch=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        data, target = batch\n","        output = self(data)\n","        loss = self.loss_fn(output, target)\n","        acc = self.accuracy(output.softmax(dim=-1), target)\n","        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n","        self.log(\"val_acc\", acc, prog_bar=True, on_epoch=True)\n","\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n","\n","\n","# DataModule for MNIST\n","class MNISTDataModule(pl.LightningDataModule):\n","    def __init__(self, batch_size=128):\n","        super().__init__()\n","        self.batch_size = batch_size\n","\n","    def prepare_data(self):\n","        # Download MNIST data\n","        datasets.MNIST(root=\"./data\", train=True, download=True)\n","        datasets.MNIST(root=\"./data\", train=False, download=True)\n","\n","    def setup(self, stage=None):\n","        # Transform\n","        transform = transforms.Compose([transforms.ToTensor()])\n","        self.train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform)\n","        self.val_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform)\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n","\n","\n","# Main function\n","def main():\n","    # Instantiate the data module and model\n","    data_module = MNISTDataModule()\n","    model = MNISTModel()\n","\n","    # Define the trainer with TPU support\n","    trainer = pl.Trainer(\n","        max_epochs=5,\n","        accelerator=\"tpu\",\n","        devices=1,  # Set this to the number of TPU cores\n","        strategy=TPUStrategy(),\n","        log_every_n_steps=10,\n","    )\n","\n","    # Train the model\n","    trainer.fit(model, data_module)\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"pkI34U3X2Slh","executionInfo":{"status":"error","timestamp":1737010247171,"user_tz":-360,"elapsed":19984,"user":{"displayName":"jabed omor","userId":"16128518018356494995"}},"outputId":"1cc039cc-4d26-4c87-b16a-0439848678ba"},"execution_count":14,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'TPUStrategy' from 'pytorch_lightning.strategies' (/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-a3eb0a04e2f7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTPUStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Define the LightningModule for MNIST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'TPUStrategy' from 'pytorch_lightning.strategies' (/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":[],"metadata":{"id":"kNi-i8b74KVb"},"execution_count":null,"outputs":[]}]}